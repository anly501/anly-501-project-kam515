{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE PACKAGES\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tweepy\n",
    "\n",
    "import json\n",
    "from logging import raiseExceptions \n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD API KEYS\n",
    "consumer_key        = 'Z3BXr1e4ApQjojRMzDGXn3Kma'\n",
    "consumer_secret     = '6N0o3oO8WwUiBF5jcjmDwjHsuCSZVyLaPfkw8CvdXoJpMzuzYj'\n",
    "access_token        = '4075947982-96AArMZgSwsjMT471Su3ItkFXc3XnEOPvZcA879'\n",
    "access_token_secret = 'Peb41wtWLS9tPJ9HqPlhJMJHAgpnMIjdsC8EbuyB3cUVZ'\n",
    "bearer_token        = 'AAAAAAAAAAAAAAAAAAAAAHsnfAEAAAAAuulr34faSC6vEP7AE%2FnLgwpUiiA%3DSsRJsgAC4GfA5lkFGqC5KkOXoNPTpyayRlpLD6Fl2K97PU8D5n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FUNCTION TO SAVE TWEEPY SEARCH RESULTS\n",
    "#   searches=array with various tweepy search objects\n",
    "\n",
    "def save_search_tweets_results(searches,info_str=\"\",output_name=\"tweet-search.json\"):\n",
    "    if(str(type(searches)) == \"<class 'list'>\"):\n",
    "        #COMBINE ALL JSONS FOR VARIOUS TWEETS INTO ON BIG JSON CALLED \"out\"\n",
    "        out={}\n",
    "        out[\"search_info\"]=info_str\n",
    "        #LOOP OVER SEARCHES\n",
    "        tweet_ids=[]\n",
    "        k=0 #counter\n",
    "        for search in searches:\n",
    "            #LOOP OVER TWEETS IN SEARCH\n",
    "            for i in range(0,len(search)):\n",
    "                out[str(k)]=search[i]._json\n",
    "                tweet_id=search[i]._json[\"id_str\"]\n",
    "                #CHECK FOR REDUNDANT TWEETS\n",
    "                if tweet_id in tweet_ids:\n",
    "                    print(\"WARNING: REPEATED TWEETS IN SAVED FILE; ID = \",tweet_id)\n",
    "                tweet_ids.append(search[i]._json[\"id_str\"])\n",
    "                k+=1\n",
    "            #pretty_print_json(out)\n",
    "\n",
    "        #DELETE FILE IF IT EXIST (START FRESH)\n",
    "        if os.path.exists(output_name):\n",
    "            os.remove(output_name)\n",
    "        #WRITE FILE\n",
    "        with open(output_name, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    else: \n",
    "        raise RuntimeError(\"ERROR: Incorrect datatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP CONNECTION\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: twitter search failed\n",
      "WARNING: twitter search failed\n",
      "WARNING: twitter search failed\n",
      "WARNING: twitter search failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# REPORT BASIC SEARCH INFO\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/Python_APIs.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(num_tweets_collected,\u001b[39mlen\u001b[39m(searches))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN SEARCH \n",
    "\n",
    "#SEARCH PARAM\n",
    "query= \"education East St. Louis\" # keywords\n",
    "fromDate = \"2012-01-01\" # Date of takeover\n",
    "toDate = \"2013-12-31\" # Date of end of takeover\n",
    "\n",
    "\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "number_of_tweets=250\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "\n",
    "#KEEP SEARCHING UNTIL DESIRED NUMBER OF TWEETS COLLECTED\n",
    "while num_tweets_collected<number_of_tweets: \n",
    "    try: \n",
    "        #FIRST SEARCH\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_full_archive(query, fromDate, toDate, count=100)\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_full_archive(query, fromDate, toDate, count=100, max_id=max_id_next)\n",
    "\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        #SAVE SEARCH RESULTS\n",
    "        searches.append(search_results)\n",
    "\n",
    "        #SAVE TEMPORARY CHECKPOINTS (DONT DO TOO OFTEN .. SLOWS CODE DOWN)\n",
    "        if(k%10==0):\n",
    "            print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "        if(k%25==0):\n",
    "            save_search_tweets_results(searches,output_name=\"tmp-snapshot.json\")\n",
    "            \n",
    "        k+=1\n",
    "    except:\n",
    "        print(\"WARNING: twitter search failed\")\n",
    "\n",
    "    #SLEEP 5 SECONDS BEFORE NEXT REQUEST \n",
    "    if(number_of_tweets>18000):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "\n",
    "# REPORT BASIC SEARCH INFO\n",
    "print(num_tweets_collected,len(searches))\n",
    "print(\"search time (s) =\", (time.time() - start_time))\n",
    "\n",
    "#TIMESTAMP SEARCH \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%Y-H%H-M%M-S%S\")\n",
    "\n",
    "# SAVE RESULTS\n",
    "info_str=\"query = \"+query+\"; number_of_tweets = \"+str(number_of_tweets)+\"; date = \"+str(dt_string)\n",
    "out_name=str(dt_string)+\"-twitter-search.json\"\n",
    "save_search_tweets_results(searches,info_str=info_str,output_name=out_name)\n",
    "\n",
    "#CLEAN-UP TEMP FILES\n",
    "os.remove(\"tmp-snapshot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 200, 'result': [{'year': 2011, 'sample': 'R3', 'yearSampleLabel': '2011', 'Cohort': 2, 'CohortLabel': 'Grade 8', 'stattype': 'MN:MN', 'subject': 'WRI', 'grade': 8, 'scale': 'WRIRP', 'jurisdiction': 'NP', 'variable': 'GENDER', 'variableLabel': 'Gender', 'varValue': '1', 'varValueLabel': 'Male', 'value': 139.099504632971, 'isStatDisplayable': 1, 'errorFlag': 0}, {'year': 2011, 'sample': 'R3', 'yearSampleLabel': '2011', 'Cohort': 2, 'CohortLabel': 'Grade 8', 'stattype': 'MN:MN', 'subject': 'WRI', 'grade': 8, 'scale': 'WRIRP', 'jurisdiction': 'NP', 'variable': 'GENDER', 'variableLabel': 'Gender', 'varValue': '2', 'varValueLabel': 'Female', 'value': 158.567104984955, 'isStatDisplayable': 1, 'errorFlag': 0}]}\n"
     ]
    }
   ],
   "source": [
    "# NAEP Data Service API\n",
    "# Documentation: https://nces.ed.gov/naep/developer/api\n",
    "\n",
    "type = 'data'\n",
    "subject = 'writing'\n",
    "grade = '8'\n",
    "subscale = 'WRIRP'\n",
    "variable = 'GENDER'\n",
    "jurisdiction = 'NP'\n",
    "Stattype = 'MN:MN'\n",
    "Year = '2011'\n",
    "\n",
    "query_params = {'type': type, 'subject': subject, 'grade': grade, 'subscale': subscale, 'variable': variable, 'stattype': Stattype, 'jurisdiction': jurisdiction, 'Year': Year}\n",
    "\n",
    "response = requests.get(\"https://www.nationsreportcard.gov/DataService/GetAdhocData.aspx?\", params=query_params)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API for those school district names + education in the years they were taken over by the state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f75bc8446c94e3689055a18017573334e0a773cb1d1589a7fad01c467977f447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
