{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "# read csv file\n",
    "\n",
    "\n",
    "urban_docs = read_csv('/Users/katherinemead/Documents/GitHub/anly-501-project-kam515/501-project-website/codes/01-data-gathering/Urban_API_Endpoints - Sheet1.csv')\n",
    "\n",
    "# level + _ + source + _ + topic + _ + year + .json\n",
    "\n",
    "# create a list of all the json files\n",
    "json_files = []\n",
    "for i in range(len(urban_docs)):\n",
    "    json_files.append(str(urban_docs['Level'][i]) + '_' + str(urban_docs['Source'][i]) + '_' + str(urban_docs['Topic'][i]) + '_')\n",
    "\n",
    "years = ['2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
    "\n",
    "new_json_files = []\n",
    "stem_names = []\n",
    "for i in json_files:\n",
    "    for year in years:\n",
    "        new_json_files.append('/Users/katherinemead/Documents/Urban_Institute_Files/' + i + (year + '.json'))\n",
    "        stem_names.append(i + year)\n",
    "\n",
    "\n",
    "\n",
    "error_list = []\n",
    "working_list = []\n",
    "working_stem_names = []\n",
    "working_counter = 0\n",
    "error_counter = 0\n",
    "indexer = 0\n",
    "for i in new_json_files:\n",
    "    indexer += 1\n",
    "    try:\n",
    "        with open(i) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            working_counter += 1\n",
    "            working_list.append(i)\n",
    "            working_stem_names.append(stem_names[indexer-1])\n",
    "    except:\n",
    "        error_list.append(i)\n",
    "        error_counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(i):\n",
    "    with open(i) as file: # Use file to refer to the file object\n",
    "        # read the file and remove all escape characters\n",
    "        data = file.read()\n",
    "        json_string = json.dumps(data, skipkeys=True)  \n",
    "        list_of_districts = json_string.split(\"{\")\n",
    "        return list_of_districts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ugly(stringy):\n",
    "    s = stringy.replace(\"\\\\n\", \"\")\n",
    "    n_string = s.replace(\"\\\\\", \"\")\n",
    "    p_string = \"{\" + n_string\n",
    "    return p_string\n",
    "\n",
    "def make_pretty_list(listy):\n",
    "    new_pretty_list = []\n",
    "    for m in listy:\n",
    "        new_pretty_list.append(remove_ugly(m))\n",
    "    return new_pretty_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass document name into function\n",
    "def spit_out_df(document):\n",
    "    list_of_jsons_for_doc = []\n",
    "    list_of_strings_that_didnt_jsonify = []\n",
    "    \n",
    "    # slashless_long_doc_string = make_pretty_list(listify(document))\n",
    "    slashless_long_doc_string = make_pretty_list(listify(document))\n",
    "    \n",
    "    # go through document and remove commas after } .replace(\"},\", \"}\") and load into json\n",
    "    district_counter = 0\n",
    "    for district_string in slashless_long_doc_string:\n",
    "        district_counter += 1\n",
    "        json_ready_string = district_string.replace(\"},\", \"}\")\n",
    "        try:\n",
    "            json_yaas = json.loads(json_ready_string)\n",
    "            list_of_jsons_for_doc.append(json_yaas)\n",
    "        except:\n",
    "            string_to_append = document + \" string number: \" + str(district_counter)\n",
    "            list_of_strings_that_didnt_jsonify.append(string_to_append)\n",
    "    \n",
    "    # Create dataframe\n",
    "    items_to_examine = []\n",
    "    for item in list_of_jsons_for_doc:\n",
    "        items_to_examine.append(item)\n",
    "    records = pd.DataFrame.from_records(items_to_examine) \n",
    "    return records, list_of_strings_that_didnt_jsonify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON DOCUMENT LEVEL\n",
    "\n",
    "# list_of_strings_that_didnt_jsonify -> loop through all docs and build a master list of all strings that didn't jsonify\n",
    "def ll_that_didnt_json(l):\n",
    "    ll_list = []\n",
    "    for document in l:\n",
    "        ready_for_master_list = spit_out_df(document)[1]\n",
    "        ll_list.append(ready_for_master_list)\n",
    "    return ll_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_docs_that_didnt_csvify = []\n",
    "doc_num = 0 \n",
    "list_of_string_level_problems = []\n",
    "s_df = pd.DataFrame()\n",
    "for doc in working_list:\n",
    "    #list_of_string_level_problems.append(ll_that_didnt_json(doc))\n",
    "    doc_num += 1\n",
    "    try:\n",
    "        df = spit_out_df(doc)[0]\n",
    "        doc_name = '/Users/katherinemead/Documents/Urban_Institute_Files/Csv_files/' + str(working_stem_names[doc_num-1]) + '.csv'\n",
    "        df.to_csv(doc_name)\n",
    "        s_problems = list_of_string_level_problems.append(spit_out_df(doc)[1])\n",
    "        s_df[doc] = s_problems\n",
    "    except:\n",
    "        list_of_docs_that_didnt_csvify.append(doc)\n",
    "s_df.to_csv('/Users/katherinemead/Documents/Urban_Institute_Files/Csv_files/String_Problems.csv')\n",
    "doc_probs = pd.DataFrame.from_records(list_of_docs_that_didnt_csvify)\n",
    "doc_probs.to_csv('/Users/katherinemead/Documents/Urban_Institute_Files/Csv_files/Doc_Problems.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f75bc8446c94e3689055a18017573334e0a773cb1d1589a7fad01c467977f447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
